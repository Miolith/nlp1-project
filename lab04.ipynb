{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4a03e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bda6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d32e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Found cached dataset imdb (/Users/karenkaspar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d64a1267e1f4cbba05b54b3cc65b344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8009f6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We do not need the \"unsupervised\" split.\n",
    "dataset.pop(\"unsupervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04ed4601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee563560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'],\n",
       " 'label': [0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541f745",
   "metadata": {},
   "source": [
    "### Pretreatment (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a404f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def pretreatment(text: str) -> str:\n",
    "    \"\"\"Clean IMDB text entries.\n",
    "    Args:\n",
    "        text: an input string.\n",
    "    Returns:\n",
    "        The cleaned text.\n",
    "    \"\"\"\n",
    "    t = text.lower()\n",
    "    t = re.sub('[{}]'.format(string.punctuation), ' ', t)\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc54127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a67ad9807f442c8566fa1c78368862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1564b81a144880968db597d43e9ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This applies the pretreatment function to all\n",
    "clean_dataset = dataset.map(lambda x: {\"text\": pretreatment(x[\"text\"]), \"label\": x[\"label\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13802e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i rented i am curious yellow from my video store because of all the controversy that surrounded it when it was first released in 1967  i also heard that at first it was seized by u s  customs if it ever tried to enter this country  therefore being a fan of films considered  controversial  i really had to see this for myself  br    br   the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life  in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states  in between asking politicians and ordinary denizens of stockholm about their opinions on politics  she has sex with her drama teacher  classmates  and married men  br    br   what kills me about i am curious yellow is that 40 years ago  this was considered pornographic  really  the sex and nudity scenes are few and far between  even then it s not shot like some cheaply made porno  while my countrymen mind find it shocking  in reality sex and nudity are a major staple in swedish cinema  even ingmar bergman  arguably their answer to good old boy john ford  had sex scenes in his films  br    br   i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america  i am curious yellow is a good film for anyone wanting to study the meat and potatoes  no pun intended  of swedish cinema  but really  this film doesn t have much of a plot ',\n",
       " ' i am curious  yellow  is a risible and pretentious steaming pile  it doesn t matter what one s political views are because this film can hardly be taken seriously on any level  as for the claim that frontal male nudity is an automatic nc 17  that isn t true  i ve seen r rated films with male nudity  granted  they only offer some fleeting views  but where are the r rated films with gaping vulvas and flapping labia  nowhere  because they don t exist  the same goes for those crappy cable shows  schlongs swinging in the breeze but not a clitoris in sight  and those pretentious indie movies like the brown bunny  in which we re treated to the site of vincent gallo s throbbing johnson  but not a trace of pink visible on chloe sevigny  before crying  or implying   double standard  in matters of nudity  the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women  there are no genitals on display when actresses appears nude  and the same cannot be said for a man  in fact  you generally won t see female genitals in an american film in anything short of porn or explicit erotica  this alleged double standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women s bodies ',\n",
       " 'if only to avoid making this type of film in the future  this film is interesting as an experiment but tells no cogent story  br    br   one might feel virtuous for sitting thru it because it touches on so many important issues but it does so without any discernable motive  the viewer comes away with no new perspectives  unless one comes up with one while one s mind wanders  as it will invariably do during this pointless film   br    br   one might better spend one s time staring out a window at a tree growing  br    br   ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset[\"train\"][\"text\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3522c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset[\"train\"][\"label\"][12490:12510]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400fd2db",
   "metadata": {},
   "source": [
    "### Train/validation split (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b94181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/karenkaspar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-a34a45cf757f28bc.arrow and /Users/karenkaspar/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-07433316c9ccd87c.arrow\n"
     ]
    }
   ],
   "source": [
    "clean_dataset_split = clean_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "\n",
    "clean_dataset_split[\"validation\"] = clean_dataset_split.pop(\"test\")\n",
    "clean_dataset_split[\"test\"] = clean_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57605c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0447dfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le dataset est environ équilibré : 9994 / 20 000\n",
      "Le dataset est environ équilibré : 2506 / 5000\n",
      "Les proportions sont environ équilibrés\n"
     ]
    }
   ],
   "source": [
    "print(\"Le dataset est environ équilibré :\", sum(clean_dataset_split[\"train\"][\"label\"]), \"/ 20 000\")\n",
    "print(\"Le dataset est environ équilibré :\", sum(clean_dataset_split[\"validation\"][\"label\"]), \"/ 5000\")\n",
    "\n",
    "print(\"Les proportions sont environ équilibrés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7044626",
   "metadata": {},
   "source": [
    "### Categorical encoding of the vocabulary (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c916ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Vocabulary manager on a collection.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"No parameters to provide.\n",
    "        \"\"\"\n",
    "        # Index to word mapping.\n",
    "        self.index2word = [PAD_TOKEN, UNK_TOKEN]\n",
    "        # Word to index mapping.\n",
    "        self.word2index = {value: key for key, value in enumerate(self.index2word)}\n",
    "        # Word counter.\n",
    "        self.word2count = defaultdict(int)\n",
    "\n",
    "    def add_word(self, word: str) -> None:\n",
    "        \"\"\"Increments the count of a word to the vocabulary.\n",
    "        Args:\n",
    "            word: the word.\n",
    "        \"\"\"\n",
    "        self.word2count[word] += 1\n",
    "        if not word in self.word2index:\n",
    "            self.word2index[word] = len(self.index2word)\n",
    "            self.index2word.append(word)\n",
    "\n",
    "    def add_text(self, text: str, separator: str =\" \") -> None:\n",
    "        \"\"\"Add the words given in a text to our vocabulary.\n",
    "        Args:\n",
    "            text: a sequence of words separated by a given separator.\n",
    "            separator: the separator used to split our text (default is \" \").\n",
    "        \"\"\"\n",
    "        for word in text.split(separator):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def get_index(self, word: str) -> int:\n",
    "        \"\"\"Returns the index of a given word in our vocabulary.\n",
    "        If the word is not in the vocabulary, returns the index for UNK_TOKEN.\n",
    "        Args:\n",
    "            word: a string.\n",
    "        Returns:\n",
    "            The corresponding index or the index for UNK_TOKEN.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.word2index[word]\n",
    "            if word in self.word2index\n",
    "            else self.word2index[UNK_TOKEN]\n",
    "        )\n",
    "\n",
    "    def get_word(self, index: int) -> str:\n",
    "        \"\"\"Returns the word at a given index in our vocabulary.\n",
    "        Args:\n",
    "            index: the word position in our vocabulary.\n",
    "        Returns:\n",
    "            The word corresponding to the given index.\n",
    "        \"\"\"\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def get_word_count(self, word: str) -> int:\n",
    "        \"\"\"Returns the number of occurences for a given word.\n",
    "        Raise a \n",
    "        Args:\n",
    "            The word.\n",
    "        Returns:\n",
    "            Its number of measured occurences.\n",
    "        \"\"\"\n",
    "        return self.word2count[word]\n",
    "\n",
    "    def get_vocabulary(self) -> List[str]:\n",
    "        \"\"\"Returns a copy of the whole vocabulary list.\n",
    "        Returns:\n",
    "            A list of words.\n",
    "        \"\"\"\n",
    "        return deepcopy(self.index2word)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"len() function.\n",
    "        Returns:\n",
    "            The number of words in the vocabulary.\n",
    "        \"\"\"\n",
    "        return len(self.index2word)\n",
    "\n",
    "    def trim_vocabulary(self, min_occurences: int = 5) -> None:\n",
    "        \"\"\"Trim the vocabulary based on the number of occurrences of each words.\n",
    "        Note that whole counts of deleted words are added to the UNK_TOKEN counts.\n",
    "        Args:\n",
    "            min_occurences: the minimum number of occurences for a word to be kept.\n",
    "        \"\"\"\n",
    "        to_delete = {\n",
    "            word for word, count in self.word2count.items() if count < min_occurences\n",
    "        }\n",
    "        new_word2count = defaultdict(int)\n",
    "        for word, count in self.word2count.items():\n",
    "            if word not in to_delete:\n",
    "                new_word2count[word] = count\n",
    "            else:\n",
    "                new_word2count[UNK_TOKEN] += count\n",
    "        new_index2word = [word for word in self.index2word if word not in to_delete]\n",
    "        new_word2index = {word: index for index, word in enumerate(new_index2word)}\n",
    "\n",
    "        self.word2count = new_word2count\n",
    "        self.index2word = new_index2word\n",
    "        self.word2index = new_word2index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2495e41",
   "metadata": {},
   "source": [
    "(1 point) Get the vocabulary on both the training and validation set using the Vocabulary class. Remember, we don't use the test set here as we consider it as proxy production data. The trim it down as you see fit (around 20K words in the vocabulary is a good value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cb87303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 19111 mots de vocabulaire différents\n"
     ]
    }
   ],
   "source": [
    "vocabulary = Vocabulary()\n",
    "\n",
    "# Your code...\n",
    "\n",
    "for text in clean_dataset[\"train\"][\"text\"]:\n",
    "    vocabulary.add_text(text)\n",
    "\n",
    "c = 5\n",
    "while len(vocabulary.word2count) > 20000:\n",
    "    vocabulary.trim_vocabulary(c)\n",
    "    c += 1\n",
    "\n",
    "print(\"Il y a\", len(vocabulary.word2count), \"mots de vocabulaire différents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc18785",
   "metadata": {},
   "source": [
    "(1 point) Fill the encoding and decoding functions. The encoding function takes a text as input and returns a list IDs corresponding to the index of each word in the vocabulary. The decoding function reverse the process, turning a list of IDs into a text. Make sure the encoding function returns a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c67eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding and decoding function\n",
    "\n",
    "def encode_text(text: str) -> np.ndarray:\n",
    "    # Your code.\n",
    "    \n",
    "    rep = []\n",
    "    for word in text.split(' '):\n",
    "        rep.append(vocabulary.word2index.get(word, vocabulary.word2index[UNK_TOKEN]))\n",
    "        \n",
    "    return np.array(rep)\n",
    "\n",
    "def decode_text(encoded_text: np.ndarray) -> str:\n",
    "    # Your code.\n",
    "    \n",
    "    return \" \".join(map((lambda x : vocabulary.index2word[x]), encoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5b543",
   "metadata": {},
   "source": [
    "To make sure everything went well, we compare a text before and after encoding and then decoding it. You should see rare words / typos replaced by the <UNK> token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9261fbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6849a5611fb244b696bd6482bbdf876f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf0a83ed67942a5a9b37891b93f865d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the encoding function to the entire dataset.\n",
    "encoded_dataset = clean_dataset.map(lambda x: {\"text\": encode_text(x[\"text\"]), \"label\": x[\"label\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "963a56f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 2,\n",
       " 26,\n",
       " 27,\n",
       " 16,\n",
       " 28,\n",
       " 21,\n",
       " 18,\n",
       " 20,\n",
       " 1,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 25,\n",
       " 32,\n",
       " 33,\n",
       " 18,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 25,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 12,\n",
       " 44,\n",
       " 45,\n",
       " 25,\n",
       " 46,\n",
       " 25,\n",
       " 2,\n",
       " 47,\n",
       " 48,\n",
       " 36,\n",
       " 49,\n",
       " 38,\n",
       " 50,\n",
       " 51,\n",
       " 25,\n",
       " 52,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 52,\n",
       " 25,\n",
       " 25,\n",
       " 14,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 42,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 36,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 25,\n",
       " 23,\n",
       " 71,\n",
       " 67,\n",
       " 64,\n",
       " 36,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 36,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 12,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 14,\n",
       " 81,\n",
       " 1,\n",
       " 82,\n",
       " 69,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 14,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 85,\n",
       " 23,\n",
       " 14,\n",
       " 92,\n",
       " 93,\n",
       " 25,\n",
       " 23,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 90,\n",
       " 97,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 69,\n",
       " 98,\n",
       " 99,\n",
       " 79,\n",
       " 100,\n",
       " 25,\n",
       " 67,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 73,\n",
       " 59,\n",
       " 104,\n",
       " 25,\n",
       " 105,\n",
       " 25,\n",
       " 90,\n",
       " 106,\n",
       " 107,\n",
       " 25,\n",
       " 52,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 52,\n",
       " 25,\n",
       " 25,\n",
       " 80,\n",
       " 108,\n",
       " 109,\n",
       " 69,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 54,\n",
       " 16,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 25,\n",
       " 38,\n",
       " 20,\n",
       " 45,\n",
       " 113,\n",
       " 25,\n",
       " 47,\n",
       " 25,\n",
       " 14,\n",
       " 102,\n",
       " 90,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 90,\n",
       " 118,\n",
       " 94,\n",
       " 25,\n",
       " 119,\n",
       " 120,\n",
       " 18,\n",
       " 31,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 76,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 25,\n",
       " 127,\n",
       " 8,\n",
       " 1,\n",
       " 128,\n",
       " 129,\n",
       " 18,\n",
       " 130,\n",
       " 25,\n",
       " 23,\n",
       " 131,\n",
       " 102,\n",
       " 90,\n",
       " 114,\n",
       " 116,\n",
       " 42,\n",
       " 132,\n",
       " 133,\n",
       " 23,\n",
       " 58,\n",
       " 134,\n",
       " 25,\n",
       " 119,\n",
       " 135,\n",
       " 136,\n",
       " 25,\n",
       " 137,\n",
       " 98,\n",
       " 138,\n",
       " 36,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 25,\n",
       " 48,\n",
       " 102,\n",
       " 115,\n",
       " 23,\n",
       " 144,\n",
       " 44,\n",
       " 25,\n",
       " 52,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 52,\n",
       " 25,\n",
       " 25,\n",
       " 2,\n",
       " 145,\n",
       " 146,\n",
       " 14,\n",
       " 147,\n",
       " 50,\n",
       " 14,\n",
       " 148,\n",
       " 16,\n",
       " 149,\n",
       " 102,\n",
       " 150,\n",
       " 23,\n",
       " 14,\n",
       " 151,\n",
       " 54,\n",
       " 150,\n",
       " 50,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 36,\n",
       " 157,\n",
       " 158,\n",
       " 90,\n",
       " 159,\n",
       " 160,\n",
       " 36,\n",
       " 161,\n",
       " 150,\n",
       " 23,\n",
       " 113,\n",
       " 162,\n",
       " 23,\n",
       " 163,\n",
       " 25,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 54,\n",
       " 42,\n",
       " 139,\n",
       " 151,\n",
       " 50,\n",
       " 164,\n",
       " 165,\n",
       " 36,\n",
       " 166,\n",
       " 14,\n",
       " 167,\n",
       " 90,\n",
       " 168,\n",
       " 25,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 25,\n",
       " 12,\n",
       " 58,\n",
       " 134,\n",
       " 25,\n",
       " 172,\n",
       " 47,\n",
       " 25,\n",
       " 38,\n",
       " 151,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 12,\n",
       " 42,\n",
       " 53,\n",
       " 25]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c234f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i rented i am curious yellow from my video store because of all the controversy that surrounded it when it was first released in 1967  i also heard that at first it was seized by u s  customs if it ever tried to enter this country  therefore being a fan of films considered  controversial  i really had to see this for myself  br    br   the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life  in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states  in between asking politicians and ordinary denizens of stockholm about their opinions on politics  she has sex with her drama teacher  classmates  and married men  br    br   what kills me about i am curious yellow is that 40 years ago  this was considered pornographic  really  the sex and nudity scenes are few and far between  even then it s not shot like some cheaply made porno  while my countrymen mind find it shocking  in reality sex and nudity are a major staple in swedish cinema  even ingmar bergman  arguably their answer to good old boy john ford  had sex scenes in his films  br    br   i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america  i am curious yellow is a good film for anyone wanting to study the meat and potatoes  no pun intended  of swedish cinema  but really  this film doesn t have much of a plot ',\n",
       " 'i rented i am curious yellow from my video store because of all the controversy that surrounded it when it was first released in 1967  i also heard that at first it was <UNK> by u s  customs if it ever tried to enter this country  therefore being a fan of films considered  controversial  i really had to see this for myself  br    br   the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life  in particular she wants to focus her attentions to making some sort of documentary on what the average <UNK> thought about certain political issues such as the vietnam war and race issues in the united states  in between asking politicians and ordinary <UNK> of <UNK> about their opinions on politics  she has sex with her drama teacher  classmates  and married men  br    br   what kills me about i am curious yellow is that 40 years ago  this was considered pornographic  really  the sex and nudity scenes are few and far between  even then it s not shot like some cheaply made porno  while my <UNK> mind find it shocking  in reality sex and nudity are a major staple in swedish cinema  even ingmar bergman  arguably their answer to good old boy john ford  had sex scenes in his films  br    br   i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america  i am curious yellow is a good film for anyone wanting to study the meat and potatoes  no pun intended  of swedish cinema  but really  this film doesn t have much of a plot ')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dataset[\"train\"][\"text\"][0], decode_text(encoded_dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d8568",
   "metadata": {},
   "source": [
    "### Batch preparation (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6244d6",
   "metadata": {},
   "source": [
    "To speed up learning, and take advantage of the GPU architecture, we provide data to the model by batches. Since all line in the same batch need to have the same length, we pad lines to the maximum length of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31af2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(X: np.ndarray, y: np.ndarray, batch_size: int = 32, pad_right: bool = False) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate randomly ordered batches of data+labels.\n",
    "    Args:\n",
    "        X: the input data.\n",
    "        y: the corresponding labels.\n",
    "        batch_size: the size of each batch [32].\n",
    "        pad_right: if true, the padding is done on the right [False].\n",
    "    \"\"\"\n",
    "    \n",
    "    X, y = shuffle(X, y)\n",
    "    n_batches = int(np.ceil(len(y) / batch_size))\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        \n",
    "        end = min((i+1)*batch_size, len(y))\n",
    "        \n",
    "        X_batch = X[i*batch_size:end]\n",
    "        y_batch = y[i*batch_size:end]\n",
    "\n",
    "        # Padding to max ength size within the batch\n",
    "        max_len = np.max([len(x) for x in X_batch])\n",
    "        for j in range(len(X_batch)):\n",
    "            x = X_batch[j]\n",
    "            pad = [vocabulary.get_index(PAD_TOKEN)] * (max_len - len(x))\n",
    "            X_batch[j] = x+pad if pad_right else pad+x\n",
    "\n",
    "        X_batch = torch.from_numpy(np.array(X_batch)).long()\n",
    "        y_batch = torch.from_numpy(np.array(y_batch)).long()\n",
    "\n",
    "        # Yielding results, so every time the function is called, it starts again from here.\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e1c3f",
   "metadata": {},
   "source": [
    "Let's see what the batches look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81519d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs tensor([[   0,    0,    0,  ...,  354,   25,   25],\n",
      "        [   0,    0,    0,  ...,   25, 1634,   25],\n",
      "        [   0,    0,    0,  ...,  196,   25,   25],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ..., 2716, 1026,   25],\n",
      "        [   0,    0,    0,  ..., 1199,   18,   25],\n",
      "        [   0,    0,    0,  ...,  126,  908,   25]]) shape: torch.Size([32, 875])\n",
      "labels tensor([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0]) shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"]):\n",
    "    print(\"inputs\", inputs, \"shape:\", inputs.shape)\n",
    "    print(\"labels\", labels, \"shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10be4a8",
   "metadata": {},
   "source": [
    "(1 point) Question: On which side should we pad the data for our use case and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77dbcaf",
   "metadata": {},
   "source": [
    "## The answer is 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d777f9b",
   "metadata": {},
   "source": [
    "### The model (13 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508bd315",
   "metadata": {},
   "source": [
    "We use a simple RNN with a configurable number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e57960f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before starting, let's set up the device. A GPU if available, else the CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0932d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"A simple RNN module with word embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_size: int, hidden_size: int, n_layers: int, n_outputs: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: vocabulary size.\n",
    "            embed_size: embedding dimensions.\n",
    "            hidden_size: hidden layer size.\n",
    "            n_layers: the number of layers.\n",
    "            n_outputs: the number of output classes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "\n",
    "        # The word embedding layer.\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        # The RNN\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = self.embed_size,\n",
    "            hidden_size = self.hidden_size,\n",
    "            num_layers = self.n_layers,\n",
    "            batch_first = True, # Changes the order of dimension to put the batches first.\n",
    "        )\n",
    "        # A fully connected layer to project the RNN's output to only one output used for classification.\n",
    "        self.fc = nn.Linear(self.hidden_size, self.n_outputs)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Function called when the model is called with data as input.\n",
    "        Args:\n",
    "            X: the input tensor of dimensions batch_size, sequence length, vocab size (actually just an int).\n",
    "        Returns:\n",
    "            The resulting tensor of dimension batch_size, sequence length, output dimensions.\n",
    "        \"\"\"\n",
    "        h0 = torch.zeros(self.n_layers, X.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out = self.embed(X)\n",
    "        # out contains the output layer of all words in the sequence.\n",
    "        # First dim is batch, second the word in the sequence, third is the vector itself.\n",
    "        # The second output value is the last vector of all intermediate layer.\n",
    "        # Only use it if you want to access the intermediate layer values of a\n",
    "        # multilayer model.\n",
    "        out, _ = self.rnn(out, h0)\n",
    "        # Getting the last value only.\n",
    "        out = out[:, -1, :]\n",
    "    \n",
    "        # Linear projection.\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8900f5",
   "metadata": {},
   "source": [
    "Note that we do not pass the output through a sigmoid function. This is because pyTorch implements some code optimization within the BCEWithLogitsLoss we'll see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fbfbd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    criterion: Callable,\n",
    "    optimizier: torch.optim.Optimizer,\n",
    "    n_epochs: int,\n",
    "    train_gen: Callable,\n",
    "    valid_gen: Callable,\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Train a model using a batch gradient descent.\n",
    "    Args:\n",
    "        model: a class inheriting from nn.Module.\n",
    "        criterion: a loss criterion.\n",
    "        optimizer: an optimizer (e.g. Adam, RMSprop, ...).\n",
    "        n_epochs: the number of training epochs.\n",
    "        train_gen: a callable function returing a batch (data, labels).\n",
    "        valid_gen: a callable function returing a batch (data, labels).\n",
    "    \"\"\"\n",
    "    train_losses = np.zeros(n_epochs)\n",
    "    valid_losses = np.zeros(n_epochs)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        t0 = datetime.now()\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        # Training loop.\n",
    "        for inputs, labels in train_gen():\n",
    "            # labels are of dimension (N,) we turn them into (N, 1).\n",
    "            labels = labels.view(-1, 1).float()\n",
    "            # Put them on the GPU.\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Reset the gradient.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizier.step()\n",
    "\n",
    "            train_loss.append(loss.item())  # .item() detach the value from GPU.\n",
    "\n",
    "        train_losses[epoch] = np.mean(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        # Evaluation loop.\n",
    "        for inputs, labels in valid_gen():\n",
    "            labels = labels.view(-1, 1).float()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            valid_loss.append(loss.item())\n",
    "\n",
    "        valid_losses[epoch] = np.mean(valid_loss)\n",
    "\n",
    "        print(f\"Epoch: {epoch}, training loss: {train_losses[epoch]}, validation loss: {valid_losses[epoch]}, in {datetime.now() - t0}\")\n",
    "\n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1d783",
   "metadata": {},
   "source": [
    "We setup the model, criterion (a binary cross entropy), and the optimizer (Adam).\n",
    "\n",
    "Note that BCEWithLogitsLoss use a mathematical trick to incorporate the sigmoid function in its computation. This trick makes the learning process go slightly faster and is the reason why we didn't put a sigmoid in the forward function of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d5c7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(len(vocabulary), 32, 64, 1, 1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82d2ed1",
   "metadata": {},
   "source": [
    "We get the 3 generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19721715",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"])\n",
    "valid_gen = lambda: data_generator(encoded_dataset[\"validation\"][\"text\"], encoded_dataset[\"validation\"][\"label\"])\n",
    "test_gen = lambda: data_generator(encoded_dataset[\"test\"][\"text\"], encoded_dataset[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a2782",
   "metadata": {},
   "source": [
    "And train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77710e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses, model = train(model, criterion, optimizer, 20, train_gen, valid_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32776ee",
   "metadata": {},
   "source": [
    "We can look at the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label=\"Training loss\")\n",
    "plt.plot(valid_losses, label=\"Validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb539dca",
   "metadata": {},
   "source": [
    "For the assignment, code the following.\n",
    "\n",
    "- (2 points) The model validation loss should go down and then up. It means the model starts overfitting after a certain number of iterations. Modify the train function so it returns the model found with the best validation loss.\n",
    "- (2 points) Add an accuracy function and report the accuracy of the training and test set.\n",
    "- (3 points) Create an LSTM class which uses an LSTM instead of an RNN. Compare its results with the RNN.\n",
    "- (2 point) Implement a function which takes any text and return the model's prediction.\\\n",
    "The function should have a string as input and return a class (0 or 1) and its probability (score out of a sigmoid).\\\n",
    "Don't forget to make the text go through the same pretreatment and encoding you used to train your model.\n",
    "- (3 points) Create a bidirectional LSTM (BiLSTM) class to classify your sentences. Report the accuracy on the training and test data.\n",
    "To combine the last output of both direction, you can concatenate, add, or max-pool them. Please document your choice.\n",
    "- (1 point) With your best classifier, look at two wrongly classified examples on the test set. Try explaining why the model was wrong.\n",
    "- (Bonus) Try finding better hyperparameters (dimensions, number of layers, ...). Document your experiments and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77c72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
